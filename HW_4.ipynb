{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тех проблемы  \n",
    "- Всё обучение было на kaggle, ссылка : https://www.kaggle.com/code/nikitamalcev99/malcev-hw-4\n",
    "- git lfs в итоге меня придушил окончательно с момента загрузки моделей, так что я разогнал инференс на локальной машине"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45b0930a"
   },
   "source": [
    "# Задание 1: сгенерировать датасет.\n",
    "\n",
    "Вам предстоит обучить микро-GPT2 модель для генерации текста. Но для начала вам\n",
    "нужно сделать датасет!\n",
    "Датасет должен состоять из текстов вашего любого писателя. Требование к датасету:\n",
    "\n",
    "*   Датасет должен быть одним txt-файлом.\n",
    "*   Выложите датасет куда-нибудь в открытый доступ и загрузите в этот ноутбук с помощью wget\n",
    "*   Датасет должен содержать по меньшей мере 500 тысяч символов. Лучше -- миллион.\n",
    "\n",
    "\n",
    "Советы:\n",
    "* www.lib.ru -- удобный сайт с txt текстов русских писателей.\n",
    "* Моделировать поэзию чуть веселее, чем прозу.\n",
    "\n",
    "## Про использованные датасеты:\n",
    "Мои датасеты:\n",
    "- input.txt - набор текстов, в большей степени за авторством Лермонтова М.Ю.\n",
    "- names.txt - набор имён, использовался для дополнительной проверки адекватности\n",
    "- add_reverse_target.txt - набор для обучения сложению\n",
    "\n",
    "\n",
    "С HF:\n",
    "- набор частей текстов из треков Славы КПСС для генерации поэзии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0166040",
    "outputId": "a4e84f17-f96f-410b-f927-adfd8771520d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'nlp_labs'...\n",
      "remote: Enumerating objects: 125, done.\u001b[K\n",
      "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
      "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
      "remote: Total 125 (delta 48), reused 87 (delta 22), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (125/125), 3.59 MiB | 4.27 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "# !wget your-link\n",
    "# Андрей - человек уважаемый, а потому имеет подписку на git, которая позволяет git lfs файлы тащить напрямую\n",
    "# но я не Андрей, поэтому кривозубые крестьяне клонируют всю репу, чтобы решить вопрос\n",
    "\n",
    "# Skip smudge - We'll download binary files later in a faster batch\n",
    "!git lfs install --skip-smudge\n",
    "\n",
    "# Do git clone here\n",
    "!git clone https://github.com/mmmhdp/nlp_labs\n",
    "\n",
    "# Fetch all the binary files in the new clone\n",
    "!git lfs pull https://github.com/mmmhdp/nlp_labs\n",
    "\n",
    "# Reinstate smudge\n",
    "!git lfs install --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTl7pxmIvLm1"
   },
   "source": [
    "# Прототип модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SkhIiLFjbLW",
    "outputId": "7b79e346-13fe-45ef-a60f-4c7a01fc9ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/nikita/.local/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/nikita/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /home/nikita/.local/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nikita/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikita/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikita/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikita/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nikita/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nikita/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/nikita/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.12.0)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/nikita/.local/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/nikita/.local/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/nikita/.local/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikita/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikita/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/nikita/.local/lib/python3.10/site-packages (1.25.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/nikita/.local/lib/python3.10/site-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/nikita/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/nikita/.local/lib/python3.10/site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /home/nikita/.local/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/nikita/.local/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/nikita/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nikita/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/nikita/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/nikita/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/nikita/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/nikita/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install --upgrade tiktoken\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "id": "fa5f3d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "id": "doztxSv6prOr"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    pass\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "#         torch.backends.cudnn.benchmark = True\n",
    "#         torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "id": "f927248b"
   },
   "outputs": [],
   "source": [
    "def get_input_path(type_def: str, platform_prefix:str = \"/kaggle/working\", path_prefix:str =\"/nlp_labs/HW_4_additional\"  ) -> str:\n",
    "    RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "    if RunningInCOLAB:\n",
    "        platform_prefix = \"/content\"\n",
    "\n",
    "    # path = platform_prefix + path_prefix\n",
    "    \n",
    "    path = \"./HW_4_additional\"\n",
    "    \n",
    "    match type_def:\n",
    "        case \"input\":\n",
    "            path += \"/input.txt\"\n",
    "        case \"names\":\n",
    "            path += \"/names.txt\"\n",
    "        case \"add\":\n",
    "            path += \"/add_reverse_target.txt\"\n",
    "        case _:\n",
    "            path = path\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "id": "OjQuRYzZm4eX"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, all_in:bool = False, task:str = \"basic\"):\n",
    "\n",
    "\n",
    "    if all_in:\n",
    "        data = torch.cat((train_data, val_data))\n",
    "    else:\n",
    "        data = train_data if split == 'train' else val_data\n",
    "\n",
    "    match task:\n",
    "        case \"add\":\n",
    "\n",
    "            valid_start_indexes = [x for x in range(0, len(data) - BLOCK_SIZE, BLOCK_SIZE)] # 16 - это длина примера в датасете\n",
    "\n",
    "            ix = torch.tensor([random.choice(valid_start_indexes) for _ in range(BATCH_SIZE)]).view(BATCH_SIZE,)\n",
    "\n",
    "            x = torch.stack(\n",
    "                [data[ i     : i + BLOCK_SIZE] for i in ix]\n",
    "            )\n",
    "\n",
    "            loss_mask = torch.where(\n",
    "                    torch.arange(BLOCK_SIZE) % 16 <= 7\n",
    "                )[0]\n",
    "\n",
    "            y = []\n",
    "\n",
    "            for i in ix:\n",
    "                y_data = torch.clone(data[i+1 : i+BLOCK_SIZE+1])\n",
    "                y_data[loss_mask] = -100 # default ignore index = -100\n",
    "                y.append(y_data)\n",
    "\n",
    "            y = torch.stack(y)\n",
    "\n",
    "        case \"basic\":\n",
    "\n",
    "            ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "\n",
    "            x = torch.stack([data[i   : i+BLOCK_SIZE  ] for i in ix])\n",
    "            y = torch.stack([data[i+1 : i+BLOCK_SIZE+1] for i in ix])\n",
    "\n",
    "        case _:\n",
    "            ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "\n",
    "            x = torch.stack([data[i   : i+BLOCK_SIZE  ] for i in ix])\n",
    "            y = torch.stack([data[i+1 : i+BLOCK_SIZE+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "id": "q1cFIBNfm6YS"
   },
   "outputs": [],
   "source": [
    "def get_data_enc_dec(from_file:bool, type_def:str, enc_dec_type:str = \"basic\"):\n",
    "\n",
    "\n",
    "    if from_file:\n",
    "        INPUT_FILE_PATH = get_input_path(type_def)\n",
    "        with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        if enc_dec_type == \"basic\":\n",
    "            chars = sorted(list(set(text)))\n",
    "            VOCAB_SIZE = len(chars)\n",
    "            stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "            itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "            encode = lambda s: [stoi[c] for c in s]\n",
    "            decode = lambda l: ''.join([itos[i] for i in l])\n",
    "        else:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            VOCAB_SIZE = encoding.max_token_value + 1\n",
    "            encode = encoding.encode\n",
    "            decode = encoding.decode\n",
    "\n",
    "        dataset = {\"train\":[{\"text\":text}]}\n",
    "\n",
    "    else:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        dataset = load_dataset(\"huggingartists/slava-kpss\")\n",
    "\n",
    "        VOCAB_SIZE = encoding.max_token_value + 1\n",
    "        encode = encoding.encode\n",
    "        decode = encoding.decode\n",
    "\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for example in dataset[\"train\"]:\n",
    "        for el in encode(example[\"text\"]):\n",
    "            data.append(el)\n",
    "\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    return data, encode, decode, VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "id": "c5a5d3e7"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "\n",
    "\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split, task=TASK, all_in=ALL_IN)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGxYiBtVuU6N"
   },
   "source": [
    "\n",
    "## Задание 2: оптимизировать модель\n",
    "Ниже приведен код обучения микро-GPT2. Прежде, чем обучать, нужно немного оптимизировать код.  Объедините `Head` и `MultiHeadAttention` в один класс, который обрабатывает все головы параллельно, рассматривая головы как еще одну размерность. Это поможет избавиться от цикла в `MultiHeadAttention` и ускорить процесс обучения. Проверьте, что ваша имплементация идентична той, что приведена ниже.\n",
    "\n",
    "### Попытка понять, а что куда в случае с доп. размерностью.\n",
    "\n",
    "\n",
    "Значения размерностей:\n",
    "$$B = 64$$ $$T = 256$$ $$H = 6$$ $$D = 64$$ $$C = H*D = 384$$\n",
    "\n",
    "\n",
    "Внимание всё ещё описывается как:\n",
    "$$\n",
    "    Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V\n",
    "$$\n",
    "\n",
    "\n",
    "Рассмотрим, как будут преобразовываться размерности:\n",
    "$$Q = (B,H,T,D)$$\n",
    "$$K = (B,H,T,D)⇒ K^{T}=(B,H,D,T)$$\n",
    "Тогда $QK$:\n",
    "$$(B,H,T,D)\\bullet(B,H,D,T)→(B,H,T,T)$$\n",
    "\n",
    "Маскируем $QK$:\n",
    "$$mask(QK) - \\text{оставит размерности неизменными}$$\n",
    "\n",
    "Далее $QK^{T}V\\frac{1}{\\sqrt{d_{k}}}$:\n",
    "$$(B,H,T,T)\\bullet(B,H,T,D)→(B,H,T,D)$$\n",
    "\n",
    "И последними шагами приведём к изначальной размерности входных данных:\n",
    "* Совмещаем последние измерения, связанные с головами, в одно:\n",
    "$$ (B,H,T,D)\\rightarrow flatten((B,H,T,D),-3,-2,-1) \\rightarrow (B,T,H*D)$$\n",
    "* Приводим размерности к $(B,T,C)$\n",
    "$$(B,T,H*D)\\bullet(H*D,C)\\rightarrow (B,T,C)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "b587ef51"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.D = head_size\n",
    "        self.H = num_heads\n",
    "        self.key = nn.Linear(N_EMBD, head_size * num_heads, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size * num_heads, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size * num_heads, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        self.proj = nn.Linear(head_size * num_heads, N_EMBD)\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "        H = self.H\n",
    "        D = self.D\n",
    "\n",
    "        k = self.key(x).view(B, H, T, D)\n",
    "        q = self.query(x).view(B, H, T, D)\n",
    "        v = self.value(x).view(B, H, T, D)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "\n",
    "        wei = wei.masked_fill_(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v\n",
    "        out = out.view(B,T,H*D)\n",
    "\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return x\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n",
    "        self.ln_f = nn.LayerNorm(N_EMBD)\n",
    "        self.lm_head = nn.Linear(N_EMBD, VOCAB_SIZE)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "2TKBtTHWt_H9"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def save_model_with_timestamp(model, suffix=\"\"):\n",
    "    torch.save(model,f\"{SOURCE_TYPE}_{datetime.now()}_{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VRr74VruXB-"
   },
   "source": [
    "\n",
    "# Задание 3: обучите модель на ваших текстах\n",
    "Используйте те параметры, которые указаны ниже и обучите модель на вашем датасете. Обучение в колабе может занять около часа. Если результаты будут выглядеть неубедительно, учите подольше. Если есть доступ к хорошей GPU, можете немного увеличить размер трансформера.\n",
    "\n",
    "\n",
    "# Задание 4: сложение\n",
    "Обучите GPT выполнять сложение двух чисел, то есть $a+b=c$. Возможно, вам будет полезно предсказать цифры числа $c$ в обратном порядке, поскольку типичный алгоритм сложения (который, как мы надеемся, GPT выучит) выполняется справа налево. Возможно, вы захотите модифицировать dataloader, чтобы он просто предоставлял случайные задачи и пропускал генерацию train и val. Возможно, вы захотите замаскировать loss на входных позициях a+b, которые просто определяют задачу, используя y=-1 в таргетах (см. CrossEntropyLoss ignore_index). Работает ли сложение? Постройте график точности сложения в зависимости от количества цифр в примере. Для этого сгенерируйте по 100 примеров с 2-мя цифрам, 3-мя и т.д.\n",
    "\n",
    "\n",
    "\n",
    "# Задание 5: калькулятор (НЕ СДЕЛАНО)\n",
    "Если ваш трасформер научился складывать (хотя бы до какого-то порядка), научите его умножать, делить и вычитать. Попробуйте обучить простой калькулятор (без скобок), в котором по одному разу может встречать каждый из знаков: *, /, +, -. Это не очень простая задача :) Возможно, вы захотите добавить в датасет chain of thoughts (https://arxiv.org/abs/2201.11903), а не только ответ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b5PybmNvkL1"
   },
   "source": [
    "## Конфигурация и тренировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "832f1ae2"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 32 # B\n",
    "BLOCK_SIZE = 256 # T\n",
    "MAX_ITERS = 2 #200_000\n",
    "EVAL_INTERVAL = 1 #10_000\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "EVAL_ITERS = 1 #1000\n",
    "N_EMBD = 384 # C\n",
    "N_HEAD = 6 # H = n_head = 6 => D = n_embd // n_head = 64 if n_embd = 384\n",
    "           # но это соотношение выбрано для удобства, размерность головы может быть выбрана иначе\n",
    "N_LAYER = 4\n",
    "DROPOUT = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "pX9obDt_mzP7"
   },
   "outputs": [],
   "source": [
    "seed_all(42)\n",
    "\n",
    "FROM_FILE = True\n",
    "ALL_IN = False\n",
    "TASK = \"basic\" # как будет выглядеть батч - с маской(для сложения) или без(для всего остального)\n",
    "SOURCE_TYPE = \"input\" # используемый датасет\n",
    "ENCODER_DECODER_TYPE = \"basic\"\n",
    "SAVE_MODEL = False\n",
    "\n",
    "data, encode, decode, VOCAB_SIZE = get_data_enc_dec(\n",
    "    from_file=FROM_FILE,\n",
    "    type_def=SOURCE_TYPE,\n",
    "    enc_dec_type=ENCODER_DECODER_TYPE\n",
    ")\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "id": "dea4888f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.34612 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(DEVICE)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "id": "6cbbac04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.3875, val loss 5.4085\n",
      "step 1: train loss 5.0596, val loss 5.0717\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        save_model_with_timestamp(model)\n",
    "\n",
    "    xb, yb = get_batch(split='train', task=TASK, all_in=ALL_IN)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    del xb\n",
    "    del yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHW8EMPoxVD8"
   },
   "source": [
    "## Проверка моделей (основная часть тренировки была проведена на kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "id": "kOa2Mzc3xY29"
   },
   "outputs": [],
   "source": [
    "# gpt2_slava = torch.load(\"/content/nlp_labs/HW_4_additional/trained_models/input_slava_final.pt\")\n",
    "# gpt2_addition_a = torch.load(\"/content/nlp_labs/HW_4_additional/trained_models/addition_basic_encoder.pt\")\n",
    "# gpt2_addition_b = torch.load(\"/content/nlp_labs/HW_4_additional/trained_models/addition_final_advanced_encoder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивный энкодер и обычный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "id": "Nuvr1zLdDnNu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как дела?\n",
      "\n",
      "- поллочишин по м?\n",
      "- Заче Паю Нучшимпнона?\n",
      "- го!\n",
      "- И Чиейсистьномня.\n",
      "\n",
      "- капололил намитакль, ню тс\n"
     ]
    }
   ],
   "source": [
    "FROM_FILE = True\n",
    "ALL_IN = False\n",
    "TASK = \"basic\" # как будет выглядеть батч - с маской(для сложения) или без(для всего остального)\n",
    "SOURCE_TYPE = \"input\" # используемый датасет\n",
    "ENCODER_DECODER_TYPE = \"basic\"\n",
    "SAVE_MODEL = False\n",
    "\n",
    "data, encode, decode, VOCAB_SIZE = get_data_enc_dec(\n",
    "    from_file=FROM_FILE,\n",
    "    type_def=SOURCE_TYPE,\n",
    "    enc_dec_type=ENCODER_DECODER_TYPE\n",
    ")\n",
    "\n",
    "gpt2 = torch.load(\n",
    "    f=\"./HW_4_additional/trained_models/input_final_basic_encoder.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "gpt2 = gpt2.to(DEVICE)\n",
    "gpt2.eval()\n",
    "\n",
    "sample = encode(\"Привет, как дела?\")\n",
    "context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "print(decode(gpt2.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### тексты Славы КПСС и encoder от openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как дела? Оли кип мне ответиан Солнить чуж, киртру, нужна хму кринка словно\n",
      "Pow-pow-pow-pie-p, вуд-ки!\n",
      "Антила заттят плати\n",
      "Я крогда, с RE-paell it!\n",
      "Pow-puziя? Олакие, и раскет клиздим!\n",
      "Давалисе, еualaд\n"
     ]
    }
   ],
   "source": [
    "FROM_FILE = False\n",
    "ALL_IN = False\n",
    "TASK = \"basic\" # как будет выглядеть батч - с маской(для сложения) или без(для всего остального)\n",
    "SOURCE_TYPE = \"input_slava\" # используемый датасет\n",
    "ENCODER_DECODER_TYPE = \"tiktoker\"\n",
    "SAVE_MODEL = False\n",
    "\n",
    "data, encode, decode, VOCAB_SIZE = get_data_enc_dec(\n",
    "    from_file=FROM_FILE,\n",
    "    type_def=SOURCE_TYPE,\n",
    "    enc_dec_type=ENCODER_DECODER_TYPE\n",
    ")\n",
    "\n",
    "gpt2 = torch.load(\n",
    "    f=\"./HW_4_additional/trained_models/input_slava_final.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "gpt2 = gpt2.to(DEVICE)\n",
    "gpt2.eval()\n",
    "\n",
    "sample = encode(\"Привет, как дела?\")\n",
    "context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "print(decode(gpt2.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### сложение с наивным энкодером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  +2  = +4\n"
     ]
    }
   ],
   "source": [
    "FROM_FILE = True\n",
    "ALL_IN = False\n",
    "TASK = \"add\" # как будет выглядеть батч - с маской(для сложения) или без(для всего остального)\n",
    "SOURCE_TYPE = \"add\" # используемый датасет\n",
    "ENCODER_DECODER_TYPE = \"basic\"\n",
    "SAVE_MODEL = False\n",
    "\n",
    "data, encode, decode, VOCAB_SIZE = get_data_enc_dec(\n",
    "    from_file=FROM_FILE,\n",
    "    type_def=SOURCE_TYPE,\n",
    "    enc_dec_type=ENCODER_DECODER_TYPE\n",
    ")\n",
    "\n",
    "gpt2 = torch.load(\n",
    "    f=\"./HW_4_additional/trained_models/addition_basic_encoder.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "gpt2 = gpt2.to(DEVICE)\n",
    "gpt2.eval()\n",
    "\n",
    "sample = encode(\"2  +2  =\")\n",
    "context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "print(decode(gpt2.generate(context, max_new_tokens=3)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "all_res = 0\n",
    "with open(\"./HW_4_additional/add.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sample = encode(line[:8])\n",
    "        target = line[8:-2] # убать \"|\" и \"\\n\"\n",
    "        \n",
    "        context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "        check = False\n",
    "        \n",
    "        for _ in range(100):\n",
    "            pred_to_test= decode(gpt2.generate(context, max_new_tokens=1)[0].tolist())[8:]\n",
    "            if target == pred_to_test[::-1]:\n",
    "                check = True\n",
    "        \n",
    "        all_res += 1\n",
    "        if check:\n",
    "            pos += 1 \n",
    "        \n",
    "print(f\"Accuracy: {pos/all_res}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### сложение с эекодером с openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  +2  =4   |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FROM_FILE = True\n",
    "ALL_IN = False\n",
    "TASK = \"add\" # как будет выглядеть батч - с маской(для сложения) или без(для всего остального)\n",
    "SOURCE_TYPE = \"add\" # используемый датасет\n",
    "ENCODER_DECODER_TYPE = \"tiktoken\"\n",
    "SAVE_MODEL = False\n",
    "\n",
    "data, encode, decode, VOCAB_SIZE = get_data_enc_dec(\n",
    "    from_file=FROM_FILE,\n",
    "    type_def=SOURCE_TYPE,\n",
    "    enc_dec_type=ENCODER_DECODER_TYPE\n",
    ")\n",
    "\n",
    "gpt2 = torch.load(\n",
    "    f=\"./HW_4_additional/trained_models/addition_final_advanced_encoder.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "gpt2 = gpt2.to(DEVICE)\n",
    "gpt2.eval()\n",
    "\n",
    "sample = encode(\"2  +2  =\")\n",
    "context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "print(decode(gpt2.generate(context, max_new_tokens = 3)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "all_res = 0\n",
    "with open(\"./HW_4_additional/add.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        sample = encode(line[:8])\n",
    "        target = line[8:-2] # убать \"|\" и \"\\n\"\n",
    "        \n",
    "        context = torch.tensor(sample ,dtype=torch.long, device=DEVICE).view(-1,len(sample))\n",
    "        check = False\n",
    "        \n",
    "        for _ in range(100):\n",
    "            pred_to_test= decode(gpt2.generate(context, max_new_tokens=1)[0].tolist())[8:]\n",
    "            if target == pred_to_test[::-1]:\n",
    "                check = True\n",
    "        \n",
    "        all_res += 1\n",
    "        if check:\n",
    "            pos += 1 \n",
    "        \n",
    "print(f\"Accuracy: {pos/all_res}\")        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11984.479376,
   "end_time": "2024-04-27T13:47:40.359336",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-27T10:27:55.87996",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
